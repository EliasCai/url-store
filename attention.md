#### 理论介绍
1. [深度学习中的Attention模型介绍及其进展](https://blog.csdn.net/jteng/article/details/52864401 )
2. [Attention Model（mechanism） 的套路](https://blog.csdn.net/BVL10101111/article/details/78470716 )
3. [目前主流的attention方法都有哪些？](https://www.zhihu.com/question/68482809/answer/264632289 )
4. [真正的完全图解Seq2Seq Attention模型](https://zhuanlan.zhihu.com/p/40920384 )
5. [深度学习对话系统理论篇--seq2seq+Attention机制模型详解](https://zhuanlan.zhihu.com/p/32092871 )
6. [Attention Model（mechanism） 的 套路 - BVL的博客 - CSDN博客](https://blog.csdn.net/BVL10101111/article/details/78470716)
7. [TensorFlow实现(十一)Attention机制 - 简书](https://www.jianshu.com/p/cc6407444a8c )
8. [自然语言处理中的自注意力机制（Self-Attention Mechanism） - 知乎](https://zhuanlan.zhihu.com/p/35041012?utm_source=ZHShareTargetIDMore&utm_medium=social&utm_oi=28954110984192 )
9. [深度 | 从各种注意力机制窥探深度学习在NLP中的神威](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650749623&idx=3&sn=9732e1f78105b5d31e77a8fc646cd302&chksm=871afec9b06d77df39eb4fab21f43504fb8cad39a487b9fd7331239b34aa97f4faaa5bc68557 )
10. [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html )


#### 代码实现
1. [《Attention is All You Need》浅读（简介+代码）](https://kexue.fm/archives/4765 )
2. [上文的代码](https://github.com/bojone/attention/blob/master/attention_keras.py )
3. [Attention mechanism Implementation for Keras](https://github.com/philipperemy/keras-attention-mechanism )
4. [Attention-based sequence to sequence learning](https://github.com/eske/seq2seq )
5. [Keras Layer that implements an Attention mechanism, with a context/query vector, for temporal data](https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2 )
6. [Keras Layer that implements an Attention mechanism for temporal data](https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d )
7. [基于注意力机制，机器之心带你理解与训练神经机器翻译系统](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650742155&idx=1&sn=137825a13a4c31fffb6b2347c0304366&chksm=871ad9f5b06d50e31e2857a08a4a9ae9f57fd0191be580952d80f1518779594670cccc903fbe )
8. [模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用 - 知乎](https://zhuanlan.zhihu.com/p/31547842 )
9. [基于TensorFlow框架的Seq2Seq英法机器翻译模型 - 知乎](https://zhuanlan.zhihu.com/p/37148308 )
10. [deep_learning_NLP/HAN_final.ipynb at master · Tixierae/deep_learning_NLP](https://github.com/Tixierae/deep_learning_NLP/blob/master/HAN/HAN_final.ipynb )
11. [google/seq2seq: A general-purpose encoder-decoder framework for Tensorflow](https://github.com/google/seq2seq )
12. [MarcBS/keras: Keras' fork with several new functionalities. Caffe2Keras converter, multimodal layers, etc.](https://github.com/MarcBS/keras )
13. [lvapeab/nmt-keras: Neural Machine Translation with Keras (Theano/Tensorflow)](https://github.com/lvapeab/nmt-keras )


#### 语音识别
1. [Attention在语音识别中的应用(1)](https://blog.csdn.net/quheDiegooo/article/details/76842201 )
5. [Attention在语音识别中的应用(2)](https://blog.csdn.net/quheDiegooo/article/details/76890128 )
3. [Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning](https://arxiv.org/abs/1609.06773 )
4. [基于seq2seq+attention的中文语音识别](https://blog.csdn.net/rxm1989/article/details/79459739 )

#### 可视化
1. [How to Visualize Your Recurrent Neural Network with Attention in Keras](https://medium.com/datalogue/attention-in-keras-1892773a4f22 )



#### 问答系统
1. [Attention机制在问答系统中的应用--attentive pooling networks - 简书](https://www.jianshu.com/p/2b3a7d0a397e )
2. [一份问答系统的小结 - 知乎](https://zhuanlan.zhihu.com/p/35667773 )

#### 阅读理解
1. [ASReader：一个经典的机器阅读理解深度学习模型_慕课手记](https://www.imooc.com/article/28709# )
2. [机器阅读理解Attention-over-Attention模型_慕课手记](https://www.imooc.com/article/29985 )
3. [深度学习解决机器阅读理解任务的研究进展 - 知乎](https://zhuanlan.zhihu.com/p/22671467 )
4. [机器阅读理解基础篇-Attention Sum Reader - 知乎](https://zhuanlan.zhihu.com/p/37645722 )
5. [阅读理解任务中的Attention-over-Attention神经网络模型原理及实现 - 呜呜哈的博客 - CSDN博客](https://blog.csdn.net/liuchonge/article/details/72848224 )
6. [机器阅读理解模型Stanford Attentive Reader源码解析 - 知乎](https://zhuanlan.zhihu.com/p/36910707 )


#### 文本分类
1. [Text Classification, Part 3 - Hierarchical attention network – Richard's deep learning blog](https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-HATN/ )
2. [Kyubyong/transformer: A TensorFlow Implementation of the Transformer: Attention Is All You Need](https://github.com/Kyubyong/transformer )
3. [Attention-based LSTM for Text Classification | Stay Hungry,Stay Foolish.](https://tobiaslee.top/2017/08/29/Attention-based-LSTM-for-Text-Classification/ )